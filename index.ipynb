{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Neural Volume Rendering and Surface Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Neural Volume Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Transmittance Calculation:\n",
    "\n",
    "<img src=\"images/part_0.png\" alt=\"Grid Visualization\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Differentiable Volume Rendering:\n",
    "\n",
    "#### 1.3 Ray Sampling\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Grid Visualization</th>\n",
    "        <th>Ray Visualization</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/1_3_grid_rays.png\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "        <td><img src=\"images/1_3_grid_vis.png\" alt=\"Ray Visualization\" width=\"400\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### 1.4 Point Sampling \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Sampled Points</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/1_4.png\" alt=\"Grid Visualization\" width=\"400\"></td>    \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### 1.5 Volume Rendering \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Visualization</th>\n",
    "        <th>Depth Map</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_1.gif\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "        <td><img src=\"images/1_5_depth_map.png\" alt=\"Ray Visualization\" width=\"400\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimizing a basic implicit volume\n",
    "\n",
    "#### 2.3 Visualization\n",
    "\n",
    "Box Center: [0.25, 0.25, 0.0]  \n",
    "\n",
    "Side Lengths: [2.01, 1.50, 1.50]\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Optimized Volume</th>\n",
    "        <th>TA Result</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_2.gif\" alt=\"Grid Visualization\" width=\"400\"></td> \n",
    "        <td><img src=\"ta_images/part_2.gif\" alt=\"Grid Visualization\" width=\"400\"></td>  \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizing a Neural Radiance Field (NeRF)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Lego Bulldozer</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_3_og.gif\" alt=\"Grid Visualization\" width=\"300\"></td>  \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NeRF Extras\n",
    "\n",
    "#### 4.1 View Dependence\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "Adding view dependence allows NeRF to model effects like reflections and specular highlights by making color depend on the viewing direction. However, this increases the risk of overfitting to the training images, making it harder for the model to generalize to unseen viewpoints. A balance must be struckâ€”too much view dependence can lead to memorization, while too little may fail to capture realistic lighting effects.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Lego Bulldozer</th>\n",
    "        <th>Materials</th>\n",
    "        <th>Materials High res</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_4_bulldozer.gif\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "        <td><img src=\"images/part_4_materials.gif\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "        <td><img src=\"images/part_4_material_highres.gif\" alt=\"Grid Visualization\"></td>  \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### 4.2 Coarse/Fine Sampling \n",
    "\n",
    "**Approach**\n",
    "\n",
    "The ray sampling implementation is inspired by the [NeuS](https://arxiv.org/pdf/2106.10689) paper. The process involves:  \n",
    "1. Sampling an initial set of points along the ray.  \n",
    "2. Using an implicit function to compute densities at each sampled point.  \n",
    "3. Resampling points along the ray based on these densities, ensuring more points are allocated to regions of higher density.  \n",
    "\n",
    "Unlike the NeRF implementation, this approach uses only one model for both coarse and fine sampling.\n",
    "\n",
    "**Trade-offs**\n",
    "\n",
    "1. Speed: The fine/coarse implementation takes 18secs/epoch while without it, it takes 13secs/epoch. Thus, fine/coarse implementation is slower, which is expected due to having to pass the sampled points twice to the model.\n",
    "\n",
    "2. Quality: For some reason, it gave a blank output, so cant really comment on the quality \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Without fine/coarse sampling</th>\n",
    "        <th>With fine/coarse sampling</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_4_bulldozer.gif\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "        <td><img src=\"images/part_4_2.gif\" alt=\"Grid Visualization\" width=\"400\"></td>  \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Surface Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Surface Tracing\n",
    "\n",
    "**Implementation:**\n",
    "My implementation starts with the origins, calculates shortest distances for each point using implicit function and marches along the ray by this distance. This process is repeated for each point for 'max_iter' (250) times. And mask is maintained for points which have distance < eps (a very small value). \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Torus</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_5_torus.gif\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optimizing a Neural SDF\n",
    "\n",
    "**MLP Model:**\n",
    "\n",
    "I am using almost the same model as used for volume rendering except for changing the relevant variables.\n",
    "The model for predicting color and distance can be explained as:\n",
    "\n",
    "1. input points are embedded to higher dimension\n",
    "2. these are then passed by 5 linear layers and relu activation\n",
    "3. the output is concatenated with embedded input points (skip connection) to give x\n",
    "4. x is passed through 4 linear layers, each followed by relu except last layer to output the distance. \n",
    "5. After step 3, x is separately passed through another 3 linear layers (relu activation, except sigmoid activation for last layer) to predict color\n",
    "\n",
    "**Eikonal Loss:**\n",
    "\n",
    "To calculate Eikonal Loss, first norm of gradient is calculated and then MSE of this norm is applied against array of ones to penalize deviations from 1.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Input</th>\n",
    "        <th>Predicted</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_6_input.gif\" alt=\"Grid Visualization\" width=\"400\"></td>\n",
    "        <td><img src=\"images/part_6.gif\" alt=\"Grid Visualization\" width=\"400\"></td>  \n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. VolSDF\n",
    "\n",
    "**beta:** Controls sharpness of the surface. Higher beta generates smoother transition and thus blurry surface while lower beta changes density more abruptly.\n",
    "\n",
    "**alpha:** Is a scaling factor that increases or decreases the overall density. Thus, higher alpha makes the surface more opaque while lower alpha give transparent effect.\n",
    "\n",
    "1. **How does high beta bias you learned SDF? What about low beta?** \n",
    "- High beta makes the density function more spread out, thus SDF transitions are smoother. This results in blurry surface representation. \n",
    "- While low beta makes density function sharper and we get sharper boundaries, it can make training less stable.\n",
    "\n",
    "2. **Would an SDF be easier to train with volume rendering and low beta or high beta? Why?**\n",
    "- It would be easier to train with high beta because smooth transitions help gradients propagate effectively during training, while low beta changes density rapidly, making optimization harder and leading to unstable updates.\n",
    "\n",
    "3. **Would you be more likely to learn an accurate surface with high beta or low beta? Why?**\n",
    "- Low beta is likely to learn accurate surface becuase it can learn sharper boundaries, thus capturing precise details.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Geometry</th>\n",
    "        <th>Best output</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_7_geometry.gif\" alt=\"Grid Visualization\" width=\"300\"></td>\n",
    "        <td><img src=\"images/part_7_normal_a_20_b_0_05.gif\" alt=\"Grid Visualization\" width=\"300\"></td>  \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "As discussed before, higher alpha gives more opaque surface, as seen between test 1 and test 2. While increasing beta gives blurrier output (test 2 -> test 3). Thus test 2 gave the best output among these.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Test conditions</th>\n",
    "        <th>alpha = 10; beta = 0.05</th>\n",
    "        <th> (Best) alpha = 20; beta = 0.05 </th>\n",
    "        <th>alpha = 20; beta = 0.5</th >\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Outputs</th>\n",
    "        <td><img src=\"images/part_7_normal_a_10_b_0.05.gif\" alt=\"Grid Visualization\" width=\"300\"></td>\n",
    "        <td><img src=\"images/part_7_normal_a_20_b_0_05.gif\" alt=\"Grid Visualization\" width=\"300\"></td>\n",
    "        <td><img src=\"images/part_7_normal_a_20_0_5.gif\" alt=\"Grid Visualization\" width=\"300\"></td>   \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Neural Surface Extras\n",
    "\n",
    "#### 8.1 Render a large scene with sphere tracing \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Scene with 4 torus in multiple locations and shapes</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_8_1_multipletorus.gif\" alt=\"Grid Visualization\" width=\"400\"></td> \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### 8.2 Fewer training views\n",
    "\n",
    "Volume and Surface training is done using 20, 50 and 100 views.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Number of views</th>\n",
    "        <th>20</th>\n",
    "        <th>50</th>\n",
    "        <th>100</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Volume Rendering</td>\n",
    "        <td><img src=\"images/part_4_normal_20views.gif\" alt=\"Grid Visualization\" width=\"250\"></td>\n",
    "        <td><img src=\"images/part_4_normal_50views.gif\" alt=\"Grid Visualization\" width=\"250\"></td> \n",
    "        <td><img src=\"images/part_4_bulldozer.gif\" alt=\"Grid Visualization\" width=\"250\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Surface Rendering</td>\n",
    "        <td><img src=\"images/part_7_naive_20views.gif\" alt=\"Grid Visualization\" width=\"250\"></td>\n",
    "        <td><img src=\"images/part_7_normal_50_views.gif\" alt=\"Grid Visualization\" width=\"250\"></td> \n",
    "        <td><img src=\"images/part_7_normal_a_20_b_0_05.gif\" alt=\"Grid Visualization\" width=\"250\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### 8.3 Alternate SDF to Density Conversions\n",
    "\n",
    "I implemented Naive gaussian conversion method to convert sdf to density based on the function:\n",
    "\n",
    "$$\n",
    "\\rho(s, \\beta) = \\exp\\left(-\\frac{s^2}{2\\beta^2}\\right)\n",
    "$$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>SDF 2 Density </th>\n",
    "        <th>Naive Guassian Conversion (beta = 0.5)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/part_7_normal_a_20_b_0_05.gif\" alt=\"Grid Visualization\" width=\"250\"></td>\n",
    "        <td><img src=\"images/part_7_naive_beta_0.5.gif\" alt=\"Grid Visualization\" width=\"250\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
